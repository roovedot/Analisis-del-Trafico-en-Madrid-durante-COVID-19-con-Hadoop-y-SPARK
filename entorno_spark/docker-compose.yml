version: '3.8'

services:
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "8080:8080"      # Web UI Master
      - "7077:7077"      # Master RPC
      - "6066:6066"      # Master REST
    environment:
      - SPARK_LOCAL_IP=0.0.0.0
      - SPARK_PUBLIC_DNS=127.0.0.1
    networks:
      - spark-net
    volumes:
      - ./outputs:/opt/spark/app/outputs  # Outputs en tu máquina
      - ./datos:/opt/spark/app/datos:ro   # Datos en read-only

  spark-worker-1:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spark-worker-1
    depends_on:
      - spark-master
    command:
      [
        "/opt/spark/bin/spark-class",
        "org.apache.spark.deploy.worker.Worker",
        "spark://spark-master:7077"
      ]
    ports:
      - "8081:8081"      # Web UI Worker
    environment:
      - SPARK_LOCAL_IP=spark-worker-1
      - SPARK_PUBLIC_DNS=127.0.0.1
    networks:
      - spark-net
    volumes:
      - ./datos:/opt/spark/app/datos:ro   # Datos en read-only

  spark-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spark-app
    depends_on:
      - spark-master
      - spark-worker-1
    command: >
      /bin/bash -c "
      echo '=== ESPERANDO A QUE SPARK MASTER ESTÉ LISTO ===' &&
      sleep 10 &&
      echo '=== EJECUTANDO ANÁLISIS ===' &&
      cd /opt/spark/app &&
      python3 analisis_spark.py 2>&1 &&
      echo '=== ANÁLISIS COMPLETADO ===' &&
      sleep 3600
      "
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_LOCAL_IP=spark-app
      - PYTHONUNBUFFERED=1
    networks:
      - spark-net
    volumes:
      - ./outputs:/opt/spark/app/outputs  # Solo outputs (lectura-escritura)

networks:
  spark-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16